{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "paraphraser_pytorch_quora.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/assaflehr/paraphraser_pytorch/blob/master/notebooks/paraphraser_pytorch_quora.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XYOemVe_MYXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "outputId": "1521d4dc-17d2-43ad-a1bf-413a668b8f8b"
      },
      "cell_type": "code",
      "source": [
        "!pip install torch -U  # 0.4 at-least, on windows, read installation instrcution of pytorch.org\n",
        "!pip install --quiet torchtext  # for simpler datasets\n",
        "!pip install  git+https://github.com/IBM/pytorch-seq2seq  #for seq2seq\n",
        "!pip install --quiet dill  #req of seq2seq\n",
        "!pip install --quiet tqdm  #req of seq2seq\n",
        "!pip install --quiet revtok #for torchtext word tokenizer\n",
        "\n",
        "#quora dataset\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"myusername\"\n",
        "os.environ['KAGGLE_KEY'] = \"mykey\"\n",
        "\n",
        "!pip install kaggle\n",
        "!kaggle competitions download -c quora-question-pairs\n",
        "!unzip train.csv.zip\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Collecting git+https://github.com/IBM/pytorch-seq2seq\n",
            "  Cloning https://github.com/IBM/pytorch-seq2seq to /tmp/pip-req-build-dyh9g0kx\n",
            "Requirement already satisfied (use --upgrade to upgrade): seq2seq==0.1.6 from git+https://github.com/IBM/pytorch-seq2seq in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (1.14.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.4.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from seq2seq==0.1.6) (0.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (4.26.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext->seq2seq==0.1.6) (2.18.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2018.8.24)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext->seq2seq==0.1.6) (2.6)\n",
            "Building wheels for collected packages: seq2seq\n",
            "  Running setup.py bdist_wheel for seq2seq ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /tmp/pip-ephem-wheel-cache-b3d51p8m/wheels/98/b5/06/771c406b3ecc8ed34f07da72d7baf65b87e561bd9f808e91bd\n",
            "Successfully built seq2seq\n",
            "Collecting kaggle\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/78/832b9a9ec6b3baf8ec566e1f0a695f2fd08d2c94a6797257a106304bfc3c/kaggle-1.4.7.1.tar.gz (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.23.0,>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.8.24)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.18.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.26.0)\n",
            "Collecting python-slugify (from kaggle)\n",
            "  Downloading https://files.pythonhosted.org/packages/00/ad/c778a6df614b6217c30fe80045b365bfa08b5dd3cb02e8b37a6d25126781/python-slugify-1.2.6.tar.gz\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.6)\n",
            "Collecting Unidecode>=0.04.16 (from python-slugify->kaggle)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K    100% |████████████████████████████████| 235kB 3.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kaggle, python-slugify\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/44/2c/df/22a6eeb780c36c28190faef6252b739fdc47145fd87a6642d4\n",
            "  Running setup.py bdist_wheel for python-slugify ... \u001b[?25l-\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e3/65/da/2045deea3098ed7471eca0e2460cfbd3fdfe8c1d6fa6fcac92\n",
            "Successfully built kaggle python-slugify\n",
            "Installing collected packages: Unidecode, python-slugify, kaggle\n",
            "Successfully installed Unidecode-1.0.22 kaggle-1.4.7.1 python-slugify-1.2.6\n",
            "Downloading test.csv.zip to /content\n",
            " 88% 153M/173M [00:05<00:00, 25.6MB/s]\n",
            "100% 173M/173M [00:05<00:00, 31.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "100% 4.95M/4.95M [00:00<00:00, 24.3MB/s]\n",
            "\n",
            "Downloading train.csv.zip to /content\n",
            " 79% 17.0M/21.5M [00:00<00:00, 17.2MB/s]\n",
            "100% 21.5M/21.5M [00:00<00:00, 25.8MB/s]\n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0bXj8NKRuPtX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "812d8299-fa28-4cb2-be6e-1a56f90e797a"
      },
      "cell_type": "code",
      "source": [
        "!rm -r paraphraser_pytorch  #remove previous github copy if needed\n",
        "!git clone https://github.com/assaflehr/paraphraser_pytorch.git\n",
        "import sys\n",
        "sys.path.append('paraphraser_pytorch/paraph')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'paraphraser_pytorch'...\n",
            "remote: Counting objects: 96, done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 96 (delta 47), reused 76 (delta 27), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (96/96), done.\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aFKCNNnp3llN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#option 1 : Run from command-line"
      ]
    },
    {
      "metadata": {
        "id": "vpc4LoyOuRTh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 635
        },
        "outputId": "e6d03576-dfd3-4877-fdc1-242d91127feb"
      },
      "cell_type": "code",
      "source": [
        "! python paraphraser_pytorch/paraph/train.py --help\n",
        "\n",
        "! python paraphraser_pytorch/paraph/train.py --dataset quora"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "usage: train.py [-h] [--dataset DATASET] [--epocs EPOCS]\n",
            "                [--epoch_size EPOCH_SIZE] [--batch_size BATCH_SIZE]\n",
            "                [--optimizer OPTIMIZER] [--lr LR] [--adv_disc_lr ADV_DISC_LR]\n",
            "                [--beta1 BETA1] [--semantics_dim SEMANTICS_DIM]\n",
            "                [--style_dim STYLE_DIM] [--sd_weight SD_WEIGHT]\n",
            "                [--sem_sim_weight SEM_SIM_WEIGHT]\n",
            "                [--max_sent_len MAX_SENT_LEN]\n",
            "\n",
            "optional arguments:\n",
            "  -h, --help            show this help message and exit\n",
            "  --dataset DATASET     dataset to use. valid values are quora or bible\n",
            "  --epocs EPOCS         number of epochs to train for\n",
            "  --epoch_size EPOCH_SIZE\n",
            "                        epoch size\n",
            "  --batch_size BATCH_SIZE\n",
            "                        batch size\n",
            "  --optimizer OPTIMIZER\n",
            "                        optimizer to train with. only Adam supported.\n",
            "  --lr LR               learning rate. 0.001 is a food value\n",
            "  --adv_disc_lr ADV_DISC_LR\n",
            "                        learning rate\n",
            "  --beta1 BETA1         momentum term for adam\n",
            "  --semantics_dim SEMANTICS_DIM\n",
            "                        size of the semantics vector.default 256\n",
            "  --style_dim STYLE_DIM\n",
            "                        size of the style vector.default 1\n",
            "  --sd_weight SD_WEIGHT\n",
            "                        weight on adversarial loss 0.0001 originally. 0.5 is\n",
            "                        good value!\n",
            "  --sem_sim_weight SEM_SIM_WEIGHT\n",
            "                        weight on semantic similiarity loss\n",
            "  --max_sent_len MAX_SENT_LEN\n",
            "                        max size of sentence. sentences typically will be\n",
            "                        shorter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Inutsgik35m8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Option 2 run from code directly"
      ]
    },
    {
      "metadata": {
        "id": "j6YTtj0vvKq9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "d064a87b-4722-49bb-908a-494ac3f33725"
      },
      "cell_type": "code",
      "source": [
        "from train import train_main\n",
        "from options import get_options\n",
        "\n",
        "sys.argv= [\"first_is_filename\",\"--dataset\",\"quora\",\"--sem_sim_weight\",\"0.01\",\"--sd_weight\",\"10\",\"--epocs\",\"100\",\"--lr\",\"0.001\"] #change if needed\n",
        "bucket_iter_train, bucket_iter_val, models= train_main()\n",
        "#now you can query the iterators, look inside the model summary, etc \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running train with options: Namespace(adv_disc_lr=0.01, batch_size=32, beta1=0.5, dataset='quora', epoch_size=500, epocs=100, lr=0.001, max_sent_len=30, optimizer='adam', sd_weight=10.0, sem_sim_weight=0.01, semantics_dim=256, style_dim=100)\n",
            "dups 149650, not_dups 413109\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' astrology ', ': ', ' i ', ' am ', ' a ', ' capricorn ', ' sun ', ' cap ', ' moon ', ' and ', ' cap ', ' rising ', '. ', '. ', '.', ' what ', ' does ', ' that ', ' say ', ' about ', ' me ', '? '], sent_1=[' i ', \"'\", ' m ', ' a ', ' triple ', ' capricorn ', ' (', ' sun ', ', ', ' moon ', ' and ', ' ascendant ', ' in ', ' capricorn ', ') ', ' what ', ' does ', ' this ', ' say ', ' about ', ' me ', '? '], sent_x=[' i ', ' want ', ' to ', ' start ', ' writing ', '. ', ' how ', ' do ', ' i ', ' begin ', '? '], is_x_0=1.0, sent_0_target=['<sos>', ' astrology ', ': ', ' i ', ' am ', ' a ', ' capricorn ', ' sun ', ' cap ', ' moon ', ' and ', ' cap ', ' rising ', '. ', '. ', '.', ' what ', ' does ', ' that ', ' say ', ' about ', ' me ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' i ', \"'\", ' m ', ' a ', ' triple ', ' capricorn ', ' (', ' sun ', ', ', ' moon ', ' and ', ' ascendant ', ' in ', ' capricorn ', ') ', ' what ', ' does ', ' this ', ' say ', ' about ', ' me ', '? '], sent_1=[' astrology ', ': ', ' i ', ' am ', ' a ', ' capricorn ', ' sun ', ' cap ', ' moon ', ' and ', ' cap ', ' rising ', '. ', '. ', '.', ' what ', ' does ', ' that ', ' say ', ' about ', ' me ', '? '], sent_x=[' how ', ' can ', ' i ', ' stop ', ' having ', ' morning ', ' diarrhea ', '? '], is_x_0=1.0, sent_0_target=['<sos>', ' i ', \"'\", ' m ', ' a ', ' triple ', ' capricorn ', ' (', ' sun ', ', ', ' moon ', ' and ', ' ascendant ', ' in ', ' capricorn ', ') ', ' what ', ' does ', ' this ', ' say ', ' about ', ' me ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? '], sent_1=[' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? '], sent_x=[' what ', ' are ', ' the ', ' funniest ', ' jokes ', ' / ', ' stories ', ' you ', ' ever ', ' heard ', '? '], is_x_0=1.0, sent_0_target=['<sos>', ' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? '], sent_1=[' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? '], sent_x=[' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? '], is_x_0=0.0, sent_0_target=['<sos>', ' what ', ' should ', ' i ', ' do ', ' to ', ' be ', ' a ', ' great ', ' geologist ', '? ', '<eos>'])\n",
            "<class 'list'> <class 'float'> OneSample(sent_0=[' how ', ' do ', ' i ', ' read ', ' and ', ' find ', ' my ', ' youtube ', ' comments ', '? '], sent_1=[' how ', ' can ', ' i ', ' see ', ' all ', ' my ', ' youtube ', ' comments ', '? '], sent_x=[' how ', ' do ', ' i ', ' read ', ' and ', ' find ', ' my ', ' youtube ', ' comments ', '? '], is_x_0=0.0, sent_0_target=['<sos>', ' how ', ' do ', ' i ', ' read ', ' and ', ' find ', ' my ', ' youtube ', ' comments ', '? ', '<eos>'])\n",
            "printing dataset directly, before tokenizing:\n",
            "sent_0 [' how ', ' can ', ' i ', ' be ', ' a ', ' good ', ' geologist ', '? ']\n",
            "is_x_0 1.0\n",
            "\n",
            "building vocab:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "09:07:45 INFO:Loading vectors from .vector_cache/wiki.simple.vec.pt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "vocab TEXT: len 29464 common [('? ', 155779), ('<sos>', 149650), ('<eos>', 149650), (' the ', 64718), (' what ', 62187), (' is ', 48178), (' how ', 40218), (' i ', 33919), (' a ', 32373), (' to ', 32224)]\n",
            "vocab TEXT_TARGET: 29464 [('? ', 155779), ('<sos>', 149650), ('<eos>', 149650), (' the ', 64718), (' what ', 62187), (' is ', 48178), (' how ', 40218), (' i ', 33919), (' a ', 32373), (' to ', 32224)]\n",
            "vocab  <sos> 2 2\n",
            "vocab  <eos> 3 3\n",
            "vocab  out-of-vocab 3 0\n",
            "training with lr 0.001\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}